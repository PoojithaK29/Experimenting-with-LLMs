{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semantic Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic chunking** is an advanced text processing technique designed to split a document into meaningful and contextually coherent segments. Unlike traditional chunking methods, which divide text based on fixed word or character counts, semantic chunking leverages language models to find natural breakpoints in the text, ensuring that each segment retains its context and meaning.\n",
    "\n",
    "**How It Overcomes Traditional Chunking:** Traditional chunking methods often break text arbitrarily, which can disrupt the flow of information and scatter related content across different chunks. This leads to difficulties in understanding and retrieving relevant information. Semantic chunking addresses this by using language models to identify logical split points, ensuring that each chunk is a semantically coherent unit. This results in more relevant and contextually accurate retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load local model (using a model from Hugging Face)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to read PDF (assuming you have a function to extract text from PDF)\n",
    "def read_pdf_to_string(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Define file path\n",
    "path = \"sample.pdf\"\n",
    "content = read_pdf_to_string(path)\n",
    "\n",
    "# Semantic Chunking setup with local model\n",
    "class LocalModelEmbeddings:\n",
    "    def embed(self, texts):\n",
    "        return model.encode(texts)\n",
    "\n",
    "# Use custom embeddings and breakpoint strategy for semantic chunking\n",
    "text_splitter = SemanticChunker(LocalModelEmbeddings(), \n",
    "                                breakpoint_threshold_type='percentile', \n",
    "                                breakpoint_threshold_amount=90)\n",
    "\n",
    "# Create semantic chunks\n",
    "docs = text_splitter.create_documents([content])\n",
    "\n",
    "# Create vector store using FAISS with local embeddings\n",
    "vectorstore = FAISS.from_documents(docs, LocalModelEmbeddings())\n",
    "\n",
    "# Create a retriever to query the chunks\n",
    "chunks_query_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Test the retriever with a query\n",
    "test_query = \"Give me overview of different tradional word embeddings\"\n",
    "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
    "print(context) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of Using Semantic Chunking:**\n",
    "\n",
    "**Improved Coherence:** Ensures that each chunk contains complete and meaningful segments of text, enhancing the relevance of retrieved information.\n",
    "Better Retrieval Accuracy: By preserving the context within chunks, retrieval systems can provide more accurate answers.\n",
    "\n",
    "**Enhanced Performance:** Downstream NLP tasks, such as question answering or summarization, perform better when processing coherent text segments.\n",
    "Adaptability: The chunking process can be fine-tuned to different types of documents and tasks by adjusting the breakpoints and thresholds.\n",
    "\n",
    "**Local Processing:** Running the process with local models eliminates dependency on external APIs, allowing for better control over the data and reducing costs.\n",
    "This method is particularly valuable for processing long and complex documents where maintaining context and meaning within each chunk is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
