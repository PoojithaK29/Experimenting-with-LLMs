{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing chunk size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**#Overview:Choosing Chunk Size in LLMs (Large Language Models)**\n",
    "\n",
    "**Definition:** Chunk size refers to the amount of text data split into smaller, manageable pieces for processing by language models.\n",
    "\n",
    "**Purpose:** Balancing between computational efficiency and maintaining context for accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.evaluation import DatasetGenerator, FaithfulnessEvaluator, RelevancyEvaluator\n",
    "from llama_index.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply nest_asyncio to avoid asyncio issues\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Directory for data\n",
    "data_dir = \"sample\"\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()\n",
    "\n",
    "# Create evaluation questions and pick k out of them\n",
    "num_eval_questions = 25\n",
    "eval_documents = documents[0:20]\n",
    "data_generator = DatasetGenerator.from_documents(eval_documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()\n",
    "k_eval_questions = random.sample(eval_questions, num_eval_questions)\n",
    "\n",
    "# Define service context for Ollama for evaluation\n",
    "llama_service_context = ServiceContext.from_defaults(llm=Ollama(model=\"llama3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Faithfulness and Relevancy Evaluators\n",
    "faithfulness_llama = FaithfulnessEvaluator(service_context=llama_service_context)\n",
    "relevancy_llama = RelevancyEvaluator(service_context=llama_service_context)\n",
    "\n",
    "# Define new prompt template for faithfulness evaluation\n",
    "faithfulness_new_prompt_template = PromptTemplate(\"\"\" Please tell if a given piece of information is directly supported by the context.\n",
    "    You need to answer with either YES or NO.\n",
    "    Answer YES if any part of the context explicitly supports the information, even if most of the context is unrelated. If the context does not explicitly support the information, answer NO. Some examples are provided below.\n",
    "\n",
    "    Information: Apple pie is generally double-crusted.\n",
    "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
    "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "    Answer: YES\n",
    "\n",
    "    Information: Apple pies taste bad.\n",
    "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
    "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "    Answer: NO\n",
    "\n",
    "    Information: Paris is the capital of France.\n",
    "    Context: This document describes a day trip in Paris. You will visit famous landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n",
    "    Answer: NO\n",
    "\n",
    "    Information: {query_str}\n",
    "    Context: {context_str}\n",
    "    Answer:\n",
    "    \"\"\")\n",
    "\n",
    "faithfulness_llama.update_prompts({\"your_prompt_key\": faithfulness_new_prompt_template})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n",
    "    \"\"\"\n",
    "    Evaluate the average response time, faithfulness, and relevancy of responses generated by the local Ollama model for a given chunk size.\n",
    "    \n",
    "    Parameters:\n",
    "    chunk_size (int): The size of data chunks being processed.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
    "    \"\"\"\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    # Create vector index using Ollama\n",
    "    llm = Ollama(model=\"llama3\")\n",
    "    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size, chunk_overlap=chunk_size//5)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents, service_context=service_context\n",
    "    )\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=5)\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        faithfulness_result = faithfulness_llama.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "        \n",
    "        relevancy_result = relevancy_llama.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different chunk sizes\n",
    "chunk_sizes = [128, 256]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, k_eval_questions)\n",
    "    print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#Advantages of Using Local Models**\n",
    "\n",
    "**Cost Efficiency:** No need for API calls to external services, reducing costs associated with usage-based pricing.\n",
    "\n",
    "**Data Privacy:** Sensitive data remains within your local environment, minimizing risks of data leakage.\n",
    "\n",
    "**Customizability:** Easier to customize and fine-tune local models to meet specific needs.\n",
    "\n",
    "**Control Over Performance:** Direct control over model performance and resource utilization.\n",
    "\n",
    "**#Disadvantages of Using Local Models**\n",
    "\n",
    "**Resource Intensive:** Requires substantial local computational resources for model inference and training.\n",
    "\n",
    "**Maintenance:** Local models need to be managed and updated, which can be time-consuming.\n",
    "\n",
    "**Scalability:** Scaling to large datasets or high volumes of requests may be challenging without robust infrastructure.\n",
    "\n",
    "**Initial Setup Complexity:** Setting up and configuring local models can be complex and require specialized knowledge.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
